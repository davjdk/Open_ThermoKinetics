Техническое задание: Модуль агрегации логов в реальном времени для solid-state-kinetics
1. Описание архитектуры для реального времени
1.1 Интеграция с существующим LoggerManager

Модуль агрегации логов будет интегрирован в существующий проект solid-state-kinetics через кастомный Handler, встраиваемый в LoggerManager. Агрегация происходит "на лету" перед записью в файл/консоль.

src/
├── gui/                    # Существующий GUI модуль
├── core/                   # Существующая основная логика
├── log_aggregator/         # НОВЫЙ модуль агрегации логов
│   ├── __init__.py
│   ├── realtime_handler.py # Кастомный Handler для LoggerManager
│   ├── buffer_manager.py   # Управление буферами в памяти
│   ├── pattern_detector.py # Детекция паттернов в потоке
│   ├── aggregation_engine.py # Движок агрегации в реальном времени
│   ├── error_expansion.py  # НОВЫЙ: Детальный анализ ошибок
│   ├── tabular_formatter.py # НОВЫЙ: Табличное представление
│   └── config.py          # Конфигурация агрегации
└── tests/
    └── test_log_aggregator/ # Тесты для нового модуля

1.2 Расширенная архитектура реального времени

Модуль состоит из 6 основных компонентов:

AggregatingHandler (встраивается в LoggerManager)
├── BufferManager         # Буферизация логов в памяти с временными окнами
├── PatternDetector       # Детекция паттернов в потоке данных
├── AggregationEngine     # Агрегация "на лету" с управлением состоянием
├── ErrorExpansionEngine  # НОВЫЙ: Детальный анализ и вывод ошибок
├── TabularFormatter      # НОВЫЙ: Табличное представление данных
└── ConfigManager         # Управление настройками агрегации

1.3 Принципы проектирования для реального времени
Минимальная инвазивность: Интеграция через Handler без изменения существующего кода
Буферизация: Управление памятью с временными окнами и лимитами
Производительность: Агрегация "на лету" без блокировки основного потока
Обратная совместимость: Возможность отключения агрегации без влияния на систему
Детализация ошибок: Автоматическое расширение контекста для WARNING/ERROR записей
Табличное представление: Структурированный вывод агрегированных данных
2. Техническое описание ключевых модулей
2.1 Модуль AggregatingHandler (realtime_handler.py)

Назначение: Кастомный Handler для интеграции в LoggerManager с поддержкой новых компонентов

import logging
import threading
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class LogRecord:
    timestamp: datetime
    level: str
    filename: str
    line_number: int
    message: str
    raw_record: logging.LogRecord

class AggregatingHandler(logging.Handler):
    """
    Кастомный Handler для агрегации логов в реальном времени.
    Встраивается в существующий LoggerManager без изменения кода.
    """
    
    def __init__(self, 
                 target_handler: logging.Handler,
                 buffer_size: int = 1000,
                 flush_interval: float = 2.0,
                 min_pattern_entries: int = 3,
                 enabled: bool = True,
                 enable_error_expansion: bool = True,
                 enable_tabular_format: bool = True,
                 error_context_lines: int = 5):
        """
        Args:
            target_handler: Оригинальный handler (console/file)
            buffer_size: Размер буфера в памяти
            flush_interval: Интервал принудительного сброса (секунды)
            min_pattern_entries: Минимум записей для агрегации
            enabled: Включить/выключить агрегацию
            enable_error_expansion: Включить детальный анализ ошибок
            enable_tabular_format: Включить табличное представление
            error_context_lines: Количество строк контекста для ошибок
        """
        super().__init__()
        self.target_handler = target_handler
        self.enabled = enabled
        self.enable_error_expansion = enable_error_expansion
        self.enable_tabular_format = enable_tabular_format
        
        # Компоненты агрегации
        self.buffer_manager = BufferManager(buffer_size, flush_interval)
        self.pattern_detector = PatternDetector(min_pattern_entries)
        self.aggregation_engine = AggregationEngine()
        
        # НОВЫЕ компоненты
        self.error_expansion_engine = ErrorExpansionEngine(error_context_lines)
        self.tabular_formatter = TabularFormatter()
        
        # Управление потоками
        self._lock = threading.RLock()
        self._flush_timer: Optional[threading.Timer] = None
        
        # Статистика
        self.stats = {
            'total_records': 0,
            'aggregated_records': 0,
            'patterns_detected': 0,
            'buffer_flushes': 0,
            'errors_expanded': 0,
            'tables_generated': 0
        }
    
    def emit(self, record: logging.LogRecord) -> None:
        """Обработка входящей записи лога"""
        if not self.enabled:
            # Прямая передача без агрегации
            self.target_handler.emit(record)
            return
        
        try:
            with self._lock:
                # Конвертация в внутренний формат
                log_record = self._convert_record(record)
                
                # Проверка на ошибку для немедленной обработки
                if self._is_error_record(log_record) and self.enable_error_expansion:
                    self._handle_error_immediately(log_record)
                    return
                
                # Добавление в буфер
                self.buffer_manager.add_record(log_record)
                self.stats['total_records'] += 1
                
                # Проверка на необходимость агрегации
                if self._should_aggregate():
                    self._process_buffer()
                
                # Установка таймера принудительного сброса
                self._schedule_flush()
                
        except Exception as e:
            # В случае ошибки - прямая передача
            self.target_handler.emit(record)
            # Логирование ошибки в отдельный handler
            self._log_error(f"Aggregation error: {e}")
    
    def _is_error_record(self, record: LogRecord) -> bool:
        """Проверка является ли запись ошибкой"""
        return record.level in ['WARNING', 'ERROR', 'CRITICAL']
    
    def _handle_error_immediately(self, record: LogRecord) -> None:
        """Немедленная обработка ошибки с расширенным контекстом"""
        expanded_record = self.error_expansion_engine.expand_error(
            record, 
            self.buffer_manager.get_recent_context()
        )
        self.target_handler.emit(expanded_record)
        self.stats['errors_expanded'] += 1
    
    def _convert_record(self, record: logging.LogRecord) -> LogRecord:
        """Конвертация LogRecord в внутренний формат"""
        return LogRecord(
            timestamp=datetime.fromtimestamp(record.created),
            level=record.levelname,
            filename=record.filename,
            line_number=record.lineno,
            message=record.getMessage(),
            raw_record=record
        )
    
    def _should_aggregate(self) -> bool:
        """Проверка условий для запуска агрегации"""
        return (
            self.buffer_manager.should_process() or
            self.buffer_manager.is_buffer_full()
        )
    
    def _process_buffer(self) -> None:
        """Обработка буфера с агрегацией и табличным форматированием"""
        # Получение записей из буфера
        records = self.buffer_manager.get_records_for_processing()
        
        if not records:
            return
        
        # Детекция паттернов
        patterns = self.pattern_detector.detect_patterns(records)
        self.stats['patterns_detected'] += len(patterns)
        
        # Агрегация
        aggregated_records = self.aggregation_engine.aggregate(records, patterns)
        self.stats['aggregated_records'] += len(aggregated_records)
        
        # Табличное форматирование (если включено)
        if self.enable_tabular_format and patterns:
            table_records = self.tabular_formatter.format_patterns_as_tables(patterns)
            aggregated_records.extend(table_records)
            self.stats['tables_generated'] += len(table_records)
        
        # Отправка агрегированных записей в target_handler
        for agg_record in aggregated_records:
            self.target_handler.emit(agg_record)
        
        self.stats['buffer_flushes'] += 1
    
    def _schedule_flush(self) -> None:
        """Планирование принудительного сброса буфера"""
        if self._flush_timer:
            self._flush_timer.cancel()
        
        self._flush_timer = threading.Timer(
            self.buffer_manager.flush_interval,
            self._force_flush
        )
        self._flush_timer.start()
    
    def _force_flush(self) -> None:
        """Принудительный сброс буфера по таймауту"""
        with self._lock:
            self._process_buffer()
    
    def close(self) -> None:
        """Закрытие handler с финальным сбросом"""
        with self._lock:
            if self._flush_timer:
                self._flush_timer.cancel()
            self._process_buffer()
            self.target_handler.close()
        super().close()
    
    def toggle_aggregation(self, enabled: bool) -> None:
        """Включение/выключение агрегации в runtime"""
        with self._lock:
            if not enabled and self.enabled:
                # Сброс буфера перед отключением
                self._process_buffer()
            self.enabled = enabled
    
    def toggle_error_expansion(self, enabled: bool) -> None:
        """Включение/выключение детализации ошибок"""
        self.enable_error_expansion = enabled
    
    def toggle_tabular_format(self, enabled: bool) -> None:
        """Включение/выключение табличного формата"""
        self.enable_tabular_format = enabled
    
    def get_stats(self) -> Dict[str, Any]:
        """Получение статистики работы"""
        with self._lock:
            compression_ratio = 0.0
            if self.stats['total_records'] > 0:
                processed = self.stats['total_records'] - len(self.buffer_manager.buffer)
                compression_ratio = 1 - (self.stats['aggregated_records'] / processed) if processed > 0 else 0
            
            return {
                **self.stats,
                'buffer_size': len(self.buffer_manager.buffer),
                'compression_ratio': compression_ratio,
                'enabled': self.enabled,
                'error_expansion_enabled': self.enable_error_expansion,
                'tabular_format_enabled': self.enable_tabular_format
            }

2.2 Модуль ErrorExpansionEngine (error_expansion.py)

Назначение: Детальный анализ и вывод ошибок с контекстом

import logging
from typing import List, Dict, Optional, Set
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import deque

@dataclass
class ErrorContext:
    """Контекст ошибки с дополнительной информацией"""
    error_record: LogRecord
    preceding_records: List[LogRecord]
    following_records: List[LogRecord]
    related_operations: List[LogRecord]
    error_trace: Optional[str] = None
    suggested_actions: List[str] = None

class ErrorExpansionEngine:
    """Движок для детального анализа и расширения ошибок"""
    
    def __init__(self, context_lines: int = 5, trace_depth: int = 10):
        self.context_lines = context_lines
        self.trace_depth = trace_depth
        
        # Кэш для отслеживания связанных операций
        self.operation_cache: deque[LogRecord] = deque(maxlen=100)
        
        # Паттерны ошибок для анализа
        self.error_patterns = {
            'file_not_found': {
                'keywords': ['file not found', 'no such file', 'cannot open'],
                'context_keywords': ['loading', 'opening', 'reading'],
                'suggestions': [
                    'Проверьте существование файла',
                    'Убедитесь в правильности пути',
                    'Проверьте права доступа'
                ]
            },
            'memory_error': {
                'keywords': ['memory error', 'out of memory', 'allocation failed'],
                'context_keywords': ['loading', 'processing', 'calculating'],
                'suggestions': [
                    'Уменьшите размер обрабатываемых данных',
                    'Закройте неиспользуемые приложения',
                    'Проверьте доступную память'
                ]
            },
            'gui_error': {
                'keywords': ['widget', 'window', 'display', 'render'],
                'context_keywords': ['updating', 'refreshing', 'drawing'],
                'suggestions': [
                    'Перезапустите GUI компонент',
                    'Проверьте состояние окна',
                    'Обновите отображение'
                ]
            },
            'calculation_error': {
                'keywords': ['division by zero', 'invalid value', 'nan', 'inf'],
                'context_keywords': ['calculating', 'computing', 'processing'],
                'suggestions': [
                    'Проверьте входные данные',
                    'Убедитесь в корректности параметров',
                    'Проверьте граничные условия'
                ]
            }
        }
        
        # Статистика
        self.stats = {
            'errors_processed': 0,
            'contexts_generated': 0,
            'traces_found': 0,
            'suggestions_provided': 0
        }
    
    def expand_error(self, error_record: LogRecord, 
                    recent_context: List[LogRecord]) -> logging.LogRecord:
        """Расширение ошибки с детальным контекстом"""
        
        # Анализ контекста ошибки
        context = self._analyze_error_context(error_record, recent_context)
        
        # Генерация расширенного сообщения
        expanded_message = self._generate_expanded_message(context)
        
        # Создание расширенной записи лога
        expanded_record = self._create_expanded_record(error_record, expanded_message, context)
        
        # Обновление статистики
        self.stats['errors_processed'] += 1
        if context.preceding_records or context.following_records:
            self.stats['contexts_generated'] += 1
        if context.error_trace:
            self.stats['traces_found'] += 1
        if context.suggested_actions:
            self.stats['suggestions_provided'] += 1
        
        return expanded_record
    
    def _analyze_error_context(self, error_record: LogRecord, 
                              recent_context: List[LogRecord]) -> ErrorContext:
        """Анализ контекста ошибки"""
        
        # Поиск предшествующих записей
        preceding_records = self._find_preceding_context(error_record, recent_context)
        
        # Поиск связанных операций
        related_operations = self._find_related_operations(error_record, recent_context)
        
        # Определение типа ошибки и генерация рекомендаций
        error_type = self._classify_error(error_record)
        suggestions = self._generate_suggestions(error_type, error_record)
        
        # Попытка извлечения трассировки
        error_trace = self._extract_error_trace(error_record, recent_context)
        
        return ErrorContext(
            error_record=error_record,
            preceding_records=preceding_records,
            following_records=[],  # В реальном времени следующих записей нет
            related_operations=related_operations,
            error_trace=error_trace,
            suggested_actions=suggestions
        )
    
    def _find_preceding_context(self, error_record: LogRecord, 
                               recent_context: List[LogRecord]) -> List[LogRecord]:
        """Поиск предшествующих записей для контекста"""
        if not recent_context:
            return []
        
        # Фильтрация записей до ошибки
        preceding = [
            record for record in recent_context 
            if record.timestamp < error_record.timestamp
        ]
        
        # Сортировка по времени и ограничение количества
        preceding.sort(key=lambda x: x.timestamp, reverse=True)
        return preceding[:self.context_lines]
    
    def _find_related_operations(self, error_record: LogRecord, 
                                recent_context: List[LogRecord]) -> List[LogRecord]:
        """Поиск связанных операций"""
        related = []
        
        # Поиск операций в том же файле
        same_file_ops = [
            record for record in recent_context
            if (record.filename == error_record.filename and 
                abs((record.timestamp - error_record.timestamp).total_seconds()) < 5)
        ]
        related.extend(same_file_ops)
        
        # Поиск операций с похожими ключевыми словами
        error_keywords = self._extract_keywords(error_record.message)
        for record in recent_context:
            record_keywords = self._extract_keywords(record.message)
            if len(error_keywords.intersection(record_keywords)) >= 2:
                related.append(record)
        
        # Удаление дубликатов и сортировка
        seen = set()
        unique_related = []
        for record in related:
            record_id = (record.timestamp, record.message)
            if record_id not in seen:
                seen.add(record_id)
                unique_related.append(record)
        
        unique_related.sort(key=lambda x: x.timestamp)
        return unique_related[:5]  # Ограничение количества
    
    def _classify_error(self, error_record: LogRecord) -> str:
        """Классификация типа ошибки"""
        message_lower = error_record.message.lower()
        
        for error_type, pattern in self.error_patterns.items():
            if any(keyword in message_lower for keyword in pattern['keywords']):
                return error_type
        
        return 'unknown'
    
    def _generate_suggestions(self, error_type: str, error_record: LogRecord) -> List[str]:
        """Генерация рекомендаций по устранению ошибки"""
        if error_type in self.error_patterns:
            base_suggestions = self.error_patterns[error_type]['suggestions'].copy()
            
            # Добавление специфичных рекомендаций
            if error_record.filename:
                base_suggestions.append(f'Проверьте код в файле {error_record.filename}:{error_record.line_number}')
            
            return base_suggestions
        
        return ['Проверьте логи для дополнительной информации']
    
    def _extract_error_trace(self, error_record: LogRecord, 
                            recent_context: List[LogRecord]) -> Optional[str]:
        """Извлечение трассировки ошибки"""
        # Поиск записей с трассировкой в недавнем контексте
        trace_records = []
        
        for record in recent_context:
            if any(keyword in record.message.lower() 
                   for keyword in ['traceback', 'stack trace', 'exception']):
                trace_records.append(record)
        
        if trace_records:
            # Объединение трассировки
            trace_lines = []
            for record in trace_records[-self.trace_depth:]:
                trace_lines.append(f"[{record.timestamp.strftime('%H:%M:%S')}] {record.message}")
            
            return '\n'.join(trace_lines)
        
        return None
    
    def _extract_keywords(self, message: str) -> Set[str]:
        """Извлечение ключевых слов из сообщения"""
        # Простое извлечение слов (можно улучшить с помощью NLP)
        words = message.lower().split()
        # Фильтрация значимых слов (длина > 3, не служебные слова)
        keywords = {
            word for word in words 
            if len(word) > 3 and word not in {'with', 'from', 'this', 'that', 'have', 'been'}
        }
        return keywords
    
    def _generate_expanded_message(self, context: ErrorContext) -> str:
        """Генерация расширенного сообщения ошибки"""
        lines = []
        
        # Заголовок ошибки
        lines.append("=" * 80)
        lines.append(f"🚨 DETAILED ERROR ANALYSIS - {context.error_record.level}")
        lines.append("=" * 80)
        
        # Основная ошибка
        lines.append(f"📍 Location: {context.error_record.filename}:{context.error_record.line_number}")
        lines.append(f"⏰ Time: {context.error_record.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"💬 Message: {context.error_record.message}")
        lines.append("")
        
        # Контекст предшествующих операций
        if context.preceding_records:
            lines.append("📋 PRECEDING CONTEXT:")
            lines.append("-" * 40)
            for i, record in enumerate(context.preceding_records[-5:], 1):
                time_diff = (context.error_record.timestamp - record.timestamp).total_seconds()
                lines.append(f"  {i}. [{record.level}] {record.message} ({time_diff:.1f}s ago)")
            lines.append("")
        
        # Связанные операции
        if context.related_operations:
            lines.append("🔗 RELATED OPERATIONS:")
            lines.append("-" * 40)
            for i, record in enumerate(context.related_operations, 1):
                lines.append(f"  {i}. [{record.level}] {record.filename}:{record.line_number} - {record.message}")
            lines.append("")
        
        # Трассировка
        if context.error_trace:
            lines.append("📊 ERROR TRACE:")
            lines.append("-" * 40)
            lines.append(context.error_trace)
            lines.append("")
        
        # Рекомендации
        if context.suggested_actions:
            lines.append("💡 SUGGESTED ACTIONS:")
            lines.append("-" * 40)
            for i, suggestion in enumerate(context.suggested_actions, 1):
                lines.append(f"  {i}. {suggestion}")
            lines.append("")
        
        lines.append("=" * 80)
        
        return '\n'.join(lines)
    
    def _create_expanded_record(self, original_record: LogRecord, 
                               expanded_message: str, 
                               context: ErrorContext) -> logging.LogRecord:
        """Создание расширенной записи лога"""
        
        # Создание нового LogRecord с расширенным сообщением
        expanded_record = logging.LogRecord(
            name=f"solid_state_kinetics.error_expanded",
            level=getattr(logging, original_record.level),
            pathname=original_record.filename,
            lineno=original_record.line_number,
            msg=expanded_message,
            args=(),
            exc_info=None
        )
        
        # Установка времени создания
        expanded_record.created = original_record.timestamp.timestamp()
        
        # Дополнительные атрибуты
        expanded_record.error_context = context
        expanded_record.original_message = original_record.message
        expanded_record.expansion_type = 'detailed_error_analysis'
        
        return expanded_record
    
    def get_stats(self) -> Dict[str, int]:
        """Получение статистики работы движка"""
        return self.stats.copy()

2.3 Модуль TabularFormatter (tabular_formatter.py)

Назначение: Табличное представление агрегированных данных

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import defaultdict
import json

@dataclass
class TableData:
    """Данные для табличного представления"""
    title: str
    headers: List[str]
    rows: List[List[str]]
    summary: Optional[str] = None
    table_type: str = 'generic'

class TabularFormatter:
    """Форматтер для табличного представления агрегированных логов"""
    
    def __init__(self, max_table_width: int = 120, max_rows_per_table: int = 20):
        self.max_table_width = max_table_width
        self.max_rows_per_table = max_rows_per_table
        
        # Статистика
        self.stats = {
            'tables_created': 0,
            'rows_formatted': 0,
            'patterns_processed': 0
        }
    
    def format_patterns_as_tables(self, patterns: List['PatternGroup']) -> List[logging.LogRecord]:
        """Форматирование паттернов в виде таблиц"""
        table_records = []
        
        for pattern in patterns:
            table_data = self._create_table_for_pattern(pattern)
            if table_data:
                table_record = self._create_table_record(table_data, pattern)
                table_records.append(table_record)
                
                self.stats['tables_created'] += 1
                self.stats['rows_formatted'] += len(table_data.rows)
        
        self.stats['patterns_processed'] += len(patterns)
        return table_records
    
    def _create_table_for_pattern(self, pattern: 'PatternGroup') -> Optional[TableData]:
        """Создание таблицы для конкретного паттерна"""
        
        if pattern.pattern_type == "plot_lines_addition":
            return self._create_plot_lines_table(pattern)
        
        elif pattern.pattern_type == "cascade_component_initialization":
            return self._create_initialization_table(pattern)
        
        elif pattern.pattern_type == "request_response_cycle":
            return self._create_request_response_table(pattern)
        
        elif pattern.pattern_type == "file_operations":
            return self._create_file_operations_table(pattern)
        
        elif pattern.pattern_type == "gui_updates":
            return self._create_gui_updates_table(pattern)
        
        else:
            return self._create_generic_table(pattern)
    
    def _create_plot_lines_table(self, pattern: 'PatternGroup') -> TableData:
        """Таблица для операций добавления линий на график"""
        headers = ["#", "Line Name", "Time", "Duration (ms)", "Status"]
        rows = []
        
        start_time = pattern.start_time
        
        for i, record in enumerate(pattern.records, 1):
            # Извлечение названия линии
            line_name = "Unknown"
            if "'" in record.message:
                try:
                    line_name = record.message.split("'")[1]
                except IndexError:
                    line_name = "Parse Error"
            
            # Расчет времени от начала операции
            time_offset = (record.timestamp - start_time).total_seconds() * 1000
            
            # Определение статуса
            status = "✅ Success" if record.level == "INFO" else f"⚠️ {record.level}"
            
            rows.append([
                str(i),
                line_name[:30],  # Ограничение длины
                f"+{time_offset:.1f}ms",
                f"{time_offset:.1f}",
                status
            ])
        
        # Ограничение количества строк
        if len(rows) > self.max_rows_per_table:
            displayed_rows = rows[:self.max_rows_per_table-1]
            displayed_rows.append([
                "...", 
                f"({len(rows) - self.max_rows_per_table + 1} more lines)", 
                "...", 
                "...", 
                "..."
            ])
            rows = displayed_rows
        
        duration = pattern.duration.total_seconds() * 1000
        summary = f"Total: {pattern.count} lines added in {duration:.1f}ms (avg: {duration/pattern.count:.1f}ms per line)"
        
        return TableData(
            title=f"📊 Plot Lines Addition Summary",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="plot_lines"
        )
    
    def _create_initialization_table(self, pattern: 'PatternGroup') -> TableData:
        """Таблица для каскадной инициализации компонентов"""
        headers = ["Step", "Component", "Time", "Duration (ms)", "Status"]
        rows = []
        
        start_time = pattern.start_time
        prev_time = start_time
        
        for i, record in enumerate(pattern.records, 1):
            # Извлечение названия компонента
            component_name = "Unknown Component"
            if "Initializing" in record.message:
                component_name = record.message.split("Initializing")[-1].strip()
            
            # Расчет времени
            step_duration = (record.timestamp - prev_time).total_seconds() * 1000
            total_time = (record.timestamp - start_time).total_seconds() * 1000
            
            # Определение статуса
            status = "✅ OK" if record.level == "INFO" else f"⚠️ {record.level}"
            
            rows.append([
                str(i),
                component_name[:25],
                f"+{total_time:.1f}ms",
                f"{step_duration:.1f}",
                status
            ])
            
            prev_time = record.timestamp
        
        total_duration = pattern.duration.total_seconds() * 1000
        summary = f"Initialization cascade: {pattern.count} components in {total_duration:.1f}ms"
        
        return TableData(
            title=f"🔧 Component Initialization Cascade",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="initialization"
        )
    
    def _create_request_response_table(self, pattern: 'PatternGroup') -> TableData:
        """Таблица для циклов запрос-ответ"""
        headers = ["#", "Operation", "Time", "Duration (ms)", "Status"]
        rows = []
        
        # Группировка записей по парам запрос-ответ
        request_response_pairs = self._group_request_response_pairs(pattern.records)
        
        for i, (request, response) in enumerate(request_response_pairs, 1):
            if response:
                duration = (response.timestamp - request.timestamp).total_seconds() * 1000
                status = "✅ Complete" if response.level == "INFO" else f"⚠️ {response.level}"
            else:
                duration = 0
                status = "⏳ Pending"
            
            # Извлечение типа операции
            operation = self._extract_operation_type(request.message)
            
            rows.append([
                str(i),
                operation[:30],
                request.timestamp.strftime("%H:%M:%S.%f")[:-3],
                f"{duration:.1f}" if duration > 0 else "N/A",
                status
            ])
        
        avg_duration = sum(
            (resp.timestamp - req.timestamp).total_seconds() * 1000 
            for req, resp in request_response_pairs if resp
        ) / len([p for p in request_response_pairs if p[1]])
        
        summary = f"Request-Response cycles: {len(request_response_pairs)} operations, avg: {avg_duration:.1f}ms"
        
        return TableData(
            title=f"🔄 Request-Response Cycles",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="request_response"
        )
    
    def _create_file_operations_table(self, pattern: 'PatternGroup') -> TableData:
        """Таблица для файловых операций"""
        headers = ["#", "Operation", "File", "Time", "Status"]
        rows = []
        
        for i, record in enumerate(pattern.records, 1):
            # Извлечение типа операции и файла
            operation_type = self._extract_file_operation_type(record.message)
            file_name = self._extract_file_name(record.message)
            
            status = "✅ Success" if record.level == "INFO" else f"⚠️ {record.level}"
            
            rows.append([
                str(i),
                operation_type,
                file_name[:25],
                record.timestamp.strftime("%H:%M:%S.%f")[:-3],
                status
            ])
        
        # Статистика по типам операций
        operation_counts = defaultdict(int)
        for record in pattern.records:
            op_type = self._extract_file_operation_type(record.message)
            operation_counts[op_type] += 1
        
        summary = f"File operations: {', '.join(f'{op}: {count}' for op, count in operation_counts.items())}"
        
        return TableData(
            title=f"📁 File Operations Summary",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="file_operations"
        )
    
    def _create_gui_updates_table(self, pattern: 'PatternGroup') -> TableData:
        """Таблица для обновлений GUI"""
        headers = ["#", "Component", "Update Type", "Time", "Duration (ms)"]
        rows = []
        
        start_time = pattern.start_time
        
        for i, record in enumerate(pattern.records, 1):
            # Извлечение компонента и типа обновления
            component = self._extract_gui_component(record.message)
            update_type = self._extract_update_type(record.message)
            
            time_offset = (record.timestamp - start_time).total_seconds() * 1000
            
            rows.append([
                str(i),
                component[:20],
                update_type[:15],
                f"+{time_offset:.1f}ms",
                f"{time_offset:.1f}"
            ])
        
        total_duration = pattern.duration.total_seconds() * 1000
        summary = f"GUI updates: {pattern.count} operations in {total_duration:.1f}ms"
        
        return TableData(
            title=f"🖥️ GUI Updates Batch",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="gui_updates"
        )
    
    def _create_generic_table(self, pattern: 'PatternGroup') -> TableData:
        """Универсальная таблица для неизвестных паттернов"""
        headers = ["#", "Level", "Message", "Time", "File:Line"]
        rows = []
        
        for i, record in enumerate(pattern.records, 1):
            rows.append([
                str(i),
                record.level,
                record.message[:40] + "..." if len(record.message) > 40 else record.message,
                record.timestamp.strftime("%H:%M:%S.%f")[:-3],
                f"{record.filename}:{record.line_number}"
            ])
        
        summary = f"Pattern: {pattern.pattern_type}, {pattern.count} records"
        
        return TableData(
            title=f"📋 Generic Pattern: {pattern.pattern_type}",
            headers=headers,
            rows=rows,
            summary=summary,
            table_type="generic"
        )
    
    def _create_table_record(self, table_data: TableData, pattern: 'PatternGroup') -> logging.LogRecord:
        """Создание LogRecord с табличными данными"""
        
        # Форматирование таблицы в ASCII
        ascii_table = self._format_ascii_table(table_data)
        
        # Создание записи лога
        table_record = logging.LogRecord(
            name="solid_state_kinetics.table",
            level=logging.INFO,
            pathname="tabular_formatter",
            lineno=0,
            msg=ascii_table,
            args=(),
            exc_info=None
        )
        
        # Установка времени
        table_record.created = pattern.start_time.timestamp()
        
        # Дополнительные атрибуты
        table_record.table_data = table_data
        table_record.pattern_type = pattern.pattern_type
        table_record.table_type = table_data.table_type
        
        return table_record
    
    def _format_ascii_table(self, table_data: TableData) -> str:
        """Форматирование таблицы в ASCII формат"""
        lines = []
        
        # Заголовок таблицы
        lines.append("")
        lines.append("┌" + "─" * (self.max_table_width - 2) + "┐")
        title_line = f"│ {table_data.title:<{self.max_table_width - 4}} │"
        lines.append(title_line)
        lines.append("├" + "─" * (self.max_table_width - 2) + "┤")
        
        # Расчет ширины колонок
        col_widths = self._calculate_column_widths(table_data)
        
        # Заголовки
        header_line = "│ " + " │ ".join(
            header.ljust(width) for header, width in zip(table_data.headers, col_widths)
        ) + " │"
        lines.append(header_line)
        
        # Разделитель
        separator = "├" + "┼".join("─" * (width + 2) for width in col_widths) + "┤"
        lines.append(separator)
        
        # Строки данных
        for row in table_data.rows:
            row_line = "│ " + " │ ".join(
                str(cell).ljust(width) for cell, width in zip(row, col_widths)
            ) + " │"
            lines.append(row_line)
        
        # Нижняя граница
        lines.append("└" + "─" * (self.max_table_width - 2) + "┘")
        
        # Сводка
        if table_data.summary:
            lines.append(f"📊 {table_data.summary}")
        
        lines.append("")
        
        return '\n'.join(lines)
    
    def _calculate_column_widths(self, table_data: TableData) -> List[int]:
        """Расчет оптимальной ширины колонок"""
        if not table_data.rows:
            return [10] * len(table_data.headers)
        
        # Начальная ширина на основе заголовков
        col_widths = [len(header) for header in table_data.headers]
        
        # Учет ширины данных
        for row in table_data.rows:
            for i, cell in enumerate(row):
                if i < len(col_widths):
                    col_widths[i] = max(col_widths[i], len(str(cell)))
        
        # Ограничение максимальной ширины
        max_col_width = 30
        col_widths = [min(width, max_col_width) for width in col_widths]
        
        # Подгонка под общую ширину таблицы
        total_width = sum(col_widths) + len(col_widths) * 3 + 1  # Учет разделителей
        if total_width > self.max_table_width:
            # Пропорциональное уменьшение
            scale_factor = (self.max_table_width - len(col_widths) * 3 - 1) / sum(col_widths)
            col_widths = [max(5, int(width * scale_factor)) for width in col_widths]
        
        return col_widths
    
    # Вспомогательные методы для извлечения информации
    
    def _group_request_response_pairs(self, records: List['LogRecord']) -> List[tuple]:
        """Группировка записей в пары запрос-ответ"""
        pairs = []
        pending_requests = []
        
        for record in records:
            message_lower = record.message.lower()
            
            if any(keyword in message_lower for keyword in ['request', 'emitting', 'handle']):
                pending_requests.append(record)
            elif any(keyword in message_lower for keyword in ['response', 'received', 'completed']):
                if pending_requests:
                    request = pending_requests.pop(0)
                    pairs.append((request, record))
                else:
                    pairs.append((record, None))
        
        # Добавление неспаренных запросов
        for request in pending_requests:
            pairs.append((request, None))
        
        return pairs
    
    def _extract_operation_type(self, message: str) -> str:
        """Извлечение типа операции из сообщения"""
        message_lower = message.lower()
        
        if 'file' in message_lower:
            return 'File Operation'
        elif 'gui' in message_lower or 'window' in message_lower:
            return 'GUI Operation'
        elif 'calculation' in message_lower or 'compute' in message_lower:
            return 'Calculation'
        elif 'network' in message_lower or 'request' in message_lower:
            return 'Network'
        else:
            return 'General'
    
    def _extract_file_operation_type(self, message: str) -> str:
        """Извлечение типа файловой операции"""
        message_lower = message.lower()
        
        if 'loading' in message_lower or 'load' in message_lower:
            return 'Load'
        elif 'saving' in message_lower or 'save' in message_lower:
            return 'Save'
        elif 'opening' in message_lower or 'open' in message_lower:
            return 'Open'
        elif 'closing' in message_lower or 'close' in message_lower:
            return 'Close'
        else:
            return 'Other'
    
    def _extract_file_name(self, message: str) -> str:
        """Извлечение имени файла из сообщения"""
        # Простое извлечение - можно улучшить с помощью регулярных выражений
        words = message.split()
        for word in words:
            if '.' in word and len(word) > 3:
                return word
        return 'Unknown'
    
    def _extract_gui_component(self, message: str) -> str:
        """Извлечение компонента GUI"""
        message_lower = message.lower()
        
        if 'window' in message_lower:
            return 'Window'
        elif 'button' in message_lower:
            return 'Button'
        elif 'plot' in message_lower or 'graph' in message_lower:
            return 'Plot'
        elif 'menu' in message_lower:
            return 'Menu'
        elif 'dialog' in message_lower:
            return 'Dialog'
        else:
            return 'Component'
    
    def _extract_update_type(self, message: str) -> str:
        """Извлечение типа обновления"""
        message_lower = message.lower()
        
        if 'refresh' in message_lower:
            return 'Refresh'
        elif 'redraw' in message_lower or 'paint' in message_lower:
            return 'Redraw'
        elif 'update' in message_lower:
            return 'Update'
        else:
            return 'Change'
    
    def get_stats(self) -> Dict[str, int]:
        """Получение статистики работы форматтера"""
        return self.stats.copy()

2.4 Обновленный модуль BufferManager (buffer_manager.py)

Назначение: Управление буферами с поддержкой контекста для ошибок

from collections import deque
from datetime import datetime, timedelta
from typing import List, Optional
import threading

class BufferManager:
    """Управление буферизацией логов в памяти с временными окнами и контекстом для ошибок"""
    
    def __init__(self, max_size: int = 1000, flush_interval: float = 2.0, 
                 context_size: int = 20):
        self.max_size = max_size
        self.flush_interval = flush_interval
        self.context_size = context_size  # Размер контекста для ошибок
        
        self.buffer: deque[LogRecord] = deque(maxlen=max_size)
        self.context_buffer: deque[LogRecord] = deque(maxlen=context_size)  # Отдельный буфер для контекста
        self.last_flush = datetime.now()
        self._lock = threading.RLock()
    
    def add_record(self, record: LogRecord) -> None:
        """Добавление записи в буфер с сохранением контекста"""
        with self._lock:
            self.buffer.append(record)
            self.context_buffer.append(record)  # Дублирование в контекстный буфер
            
            # Автоматическая очистка старых записей
            self._cleanup_old_records()
    
    def get_recent_context(self, max_records: int = None) -> List[LogRecord]:
        """Получение недавнего контекста для анализа ошибок"""
        with self._lock:
            max_records = max_records or self.context_size
            return list(self.context_buffer)[-max_records:]
    
    def should_process(self) -> bool:
        """Проверка необходимости обработки буфера"""
        with self._lock:
            time_threshold = datetime.now() - timedelta(seconds=self.flush_interval)
            return (
                len(self.buffer) >= 10 or  # Минимальный размер для обработки
                self.last_flush < time_threshold
            )
    
    def is_buffer_full(self) -> bool:
        """Проверка заполненности буфера"""
        return len(self.buffer) >= self.max_size * 0.8  # 80% заполнения
    
    def get_records_for_processing(self) -> List[LogRecord]:
        """Получение записей для обработки с очисткой буфера"""
        with self._lock:
            if not self.buffer:
                return []
            
            # Копирование всех записей
            records = list(self.buffer)
            
            # Очистка основного буфера (контекстный буфер остается)
            self.buffer.clear()
            self.last_flush = datetime.now()
            
            return records
    
    def _cleanup_old_records(self) -> None:
        """Очистка старых записей (старше 30 секунд)"""
        if not self.buffer:
            return
        
        cutoff_time = datetime.now() - timedelta(seconds=30)
        
        # Очистка основного буфера
        while self.buffer and self.buffer[0].timestamp < cutoff_time:
            self.buffer.popleft()
        
        # Очистка контекстного буфера (более короткий период)
        context_cutoff = datetime.now() - timedelta(seconds=10)
        while self.context_buffer and self.context_buffer[0].timestamp < context_cutoff:
            self.context_buffer.popleft()
    
    def get_buffer_info(self) -> dict:
        """Информация о состоянии буфера"""
        with self._lock:
            oldest_record = self.buffer[0].timestamp if self.buffer else None
            newest_record = self.buffer[-1].timestamp if self.buffer else None
            
            return {
                'size': len(self.buffer),
                'max_size': self.max_size,
                'context_size': len(self.context_buffer),
                'max_context_size': self.context_size,
                'oldest_record': oldest_record,
                'newest_record': newest_record,
                'last_flush': self.last_flush
            }

2.5 Обновленный модуль PatternDetector (pattern_detector.py)

Назначение: Детекция паттернов с улучшенной классификацией для табличного представления

from typing import List, Dict, Set
from datetime import timedelta
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class PatternGroup:
    pattern_type: str
    records: List[LogRecord]
    start_time: datetime
    end_time: datetime
    metadata: Dict[str, Any] = None  # Дополнительные метаданные для табличного форматирования
    
    @property
    def duration(self) -> timedelta:
        return self.end_time - self.start_time
    
    @property
    def count(self) -> int:
        return len(self.records)

class PatternDetector:
    """Детектор паттернов в потоке логов реального времени с улучшенной классификацией"""
    
    def __init__(self, min_entries: int = 3, time_window_seconds: int = 5):
        self.min_entries = min_entries
        self.time_window = timedelta(seconds=time_window_seconds)
        
        # Состояние для каскадных операций
        self.cascade_state: Dict[str, List[LogRecord]] = defaultdict(list)
        self.cascade_timeouts: Dict[str, datetime] = {}
        
        # Расширенные паттерны для лучшего табличного представления
        self.enhanced_patterns = {
            'plot_lines_addition': {
                'keywords': ['adding a new line', 'plot line', 'line added'],
                'context_keywords': ['plot', 'graph', 'chart'],
                'table_suitable': True,
                'priority': 'high'
            },
            'cascade_component_initialization': {
                'keywords': ['initializing', 'init', 'startup'],
                'context_keywords': ['component', 'module', 'system'],
                'table_suitable': True,
                'priority': 'high'
            },
            'request_response_cycle': {
                'keywords': ['handle_request', 'emitting request', 'response received', 'request completed'],
                'context_keywords': ['http', 'api', 'service'],
                'table_suitable': True,
                'priority': 'medium'
            },
            'file_operations': {
                'keywords': ['loading', 'saving', 'selected file', 'file opened', 'file closed'],
                'context_keywords': ['file', 'path', 'directory'],
                'table_suitable': True,
                'priority': 'medium'
            },
            'gui_updates': {
                'keywords': ['updating', 'refreshing', 'redraw', 'paint', 'render'],
                'context_keywords': ['gui', 'window', 'widget'],
                'table_suitable': True,
                'priority': 'low'
            },
            'error_sequences': {
                'keywords': ['error', 'exception', 'failed', 'warning'],
                'context_keywords': ['traceback', 'stack', 'debug'],
                'table_suitable': False,  # Ошибки обрабатываются отдельно
                'priority': 'critical'
            }
        }
    
    def detect_patterns(self, records: List[LogRecord]) -> List[PatternGroup]:
        """Детекция паттернов в списке записей с метаданными для табличного форматирования"""
        if not records:
            return []
        
        patterns = []
        
        # Сортировка по времени
        sorted_records = sorted(records, key=lambda x: x.timestamp)
        
        # Группировка по типам паттернов
        pattern_groups = self._group_by_enhanced_patterns(sorted_records)
        
        # Обработка каждой группы
        for pattern_type, group_records in pattern_groups.items():
            pattern_config = self.enhanced_patterns.get(pattern_type, {})
            
            if pattern_type.startswith('cascade_'):
                # Обработка каскадных операций
                cascade_patterns = self._process_cascade_pattern(pattern_type, group_records)
                patterns.extend(cascade_patterns)
            else:
                # Обработка обычных паттернов
                time_groups = self._split_by_time_window(group_records)
                for time_group in time_groups:
                    if len(time_group) >= self.min_entries:
                        # Генерация метаданных для табличного форматирования
                        metadata = self._generate_pattern_metadata(pattern_type, time_group)
                        
                        patterns.append(PatternGroup(
                            pattern_type=pattern_type,
                            records=time_group,
                            start_time=time_group[0].timestamp,
                            end_time=time_group[-1].timestamp,
                            metadata=metadata
                        ))
        
        # Сортировка по приоритету и времени
        patterns.sort(key=lambda p: (
            self._get_pattern_priority(p.pattern_type),
            p.start_time
        ))
        
        return patterns
    
    def _group_by_enhanced_patterns(self, records: List[LogRecord]) -> Dict[str, List[LogRecord]]:
        """Группировка записей по расширенным типам паттернов"""
        groups = defaultdict(list)
        
        for record in records:
            pattern_key = self._get_enhanced_pattern_key(record)
            groups[pattern_key].append(record)
        
        return groups
    
    def _get_enhanced_pattern_key(self, record: LogRecord) -> str:
        """Определение ключа паттерна для записи с учетом расширенных паттернов"""
        message = record.message.lower()
        
        # Проверка каждого паттерна
        for pattern_type, config in self.enhanced_patterns.items():
            if any(keyword in message for keyword in config['keywords']):
                # Дополнительная проверка контекстных ключевых слов
                if config.get('context_keywords'):
                    if any(ctx_keyword in message for ctx_keyword in config['context_keywords']):
                        return pattern_type
                else:
                    return pattern_type
        
        # Специальная обработка для каскадных операций
        if "initializing" in message:
            return "cascade_component_initialization"
        
        return "other"
    
    def _generate_pattern_metadata(self, pattern_type: str, records: List[LogRecord]) -> Dict[str, Any]:
        """Генерация метаданных для паттерна"""
        metadata = {
            'table_suitable': self.enhanced_patterns.get(pattern_type, {}).get('table_suitable', False),
            'priority': self.enhanced_patterns.get(pattern_type, {}).get('priority', 'low'),
            'record_count': len(records),
            'duration_ms': (records[-1].timestamp - records[0].timestamp).total_seconds() * 1000,
            'files_involved': list(set(record.filename for record in records)),
            'levels_distribution': self._calculate_level_distribution(records)
        }
        
        # Специфичные метаданные для разных типов паттернов
        if pattern_type == "plot_lines_addition":
            metadata.update(self._extract_plot_lines_metadata(records))
        elif pattern_type == "file_operations":
            metadata.update(self._extract_file_operations_metadata(records))
        elif pattern_type == "request_response_cycle":
            metadata.update(self._extract_request_response_metadata(records))
        
        return metadata
    
    def _extract_plot_lines_metadata(self, records: List[LogRecord]) -> Dict[str, Any]:
        """Извлечение метаданных для операций с линиями графика"""
        line_names = []
        for record in records:
            if "'" in record.message:
                try:
                    line_name = record.message.split("'")[1]
                    line_names.append(line_name)
                except IndexError:
                    pass
        
        return {
            'line_names': line_names,
            'unique_lines': len(set(line_names)),
            'avg_time_per_line': (records[-1].timestamp - records[0].timestamp).total_seconds() * 1000 / len(records) if records else 0
        }
    
    def _extract_file_operations_metadata(self, records: List[LogRecord]) -> Dict[str, Any]:
        """Извлечение метаданных для файловых операций"""
        operations = defaultdict(int)
        files = set()
        
        for record in records:
            message_lower = record.message.lower()
            
            if 'loading' in message_lower:
                operations['load'] += 1
            elif 'saving' in message_lower:
                operations['save'] += 1
            elif 'opening' in message_lower:
                operations['open'] += 1
            
            # Попытка извлечения имени файла
            words = record.message.split()
            for word in words:
                if '.' in word and len(word) > 3:
                    files.add(word)
        
        return {
            'operation_types': dict(operations),
            'files_affected': list(files),
            'total_operations': sum(operations.values())
        }
    
    def _extract_request_response_metadata(self, records: List[LogRecord]) -> Dict[str, Any]:
        """Извлечение метаданных для циклов запрос-ответ"""
        requests = 0
        responses = 0
        
        for record in records:
            message_lower = record.message.lower()
            if any(keyword in message_lower for keyword in ['request', 'emitting']):
                requests += 1
            elif any(keyword in message_lower for keyword in ['response', 'received']):
                responses += 1
        
        return {
            'requests_count': requests,
            'responses_count': responses,
            'completion_ratio': responses / requests if requests > 0 else 0
        }
    
    def _calculate_level_distribution(self, records: List[LogRecord]) -> Dict[str, int]:
        """Расчет распределения уровней логирования"""
        distribution = defaultdict(int)
        for record in records:
            distribution[record.level] += 1
        return dict(distribution)
    
    def _get_pattern_priority(self, pattern_type: str) -> int:
        """Получение числового приоритета паттерна для сортировки"""
        priority_map = {
            'critical': 0,
            'high': 1,
            'medium': 2,
            'low': 3
        }
        
        priority = self.enhanced_patterns.get(pattern_type, {}).get('priority', 'low')
        return priority_map.get(priority, 3)
    
    def _process_cascade_pattern(self, pattern_type: str, records: List[LogRecord]) -> List[PatternGroup]:
        """Обработка каскадных паттернов с метаданными"""
        patterns = []
        
        # Группировка по каскадным последовательностям
        cascade_groups = self._detect_cascade_sequences(records)
        
        for cascade_group in cascade_groups:
            if len(cascade_group) >= self.min_entries:
                metadata = self._generate_cascade_metadata(cascade_group)
                
                patterns.append(PatternGroup(
                    pattern_type=pattern_type,
                    records=cascade_group,
                    start_time=cascade_group[0].timestamp,
                    end_time=cascade_group[-1].timestamp,
                    metadata=metadata
                ))
        
        return patterns
    
    def _generate_cascade_metadata(self, records: List[LogRecord]) -> Dict[str, Any]:
        """Генерация метаданных для каскадных операций"""
        components = []
        for record in records:
            if "Initializing" in record.message:
                comp_name = record.message.split("Initializing")[-1].strip()
                components.append(comp_name)
        
        return {
            'table_suitable': True,
            'priority': 'high',
            'components': components,
            'cascade_depth': len(components),
            'avg_step_time': (records[-1].timestamp - records[0].timestamp).total_seconds() * 1000 / len(records) if records else 0
        }
    
    def _detect_cascade_sequences(self, records: List[LogRecord]) -> List[List[LogRecord]]:
        """Детекция каскадных последовательностей"""
        sequences = []
        current_sequence = []
        
        for record in records:
            if not current_sequence:
                current_sequence = [record]
            else:
                # Проверка временной близости
                time_diff = record.timestamp - current_sequence[-1].timestamp
                if time_diff <= timedelta(seconds=1):  # Каскад в пределах 1 секунды
                    current_sequence.append(record)
                else:
                    if len(current_sequence) >= self.min_entries:
                        sequences.append(current_sequence)
                    current_sequence = [record]
        
        # Добавление последней последовательности
        if len(current_sequence) >= self.min_entries:
            sequences.append(current_sequence)
        
        return sequences
    
    def _split_by_time_window(self, records: List[LogRecord]) -> List[List[LogRecord]]:
        """Разделение записей по временным окнам"""
        if not records:
            return []
        
        groups = []
        current_group = [records[0]]
        
        for record in records[1:]:
            if record.timestamp - current_group[-1].timestamp <= self.time_window:
                current_group.append(record)
            else:
                groups.append(current_group)
                current_group = [record]
        
        groups.append(current_group)
        return groups

2.6 Обновленный модуль AggregationEngine (aggregation_engine.py)

Назначение: Движок агрегации с поддержкой табличного форматирования

import logging
from typing import List, Union
from datetime import datetime

class AggregatedLogRecord(logging.LogRecord):
    """Кастомный LogRecord для агрегированных записей"""
    
    def __init__(self, summary: str, original_records: List[LogRecord], 
                 pattern_type: str, start_time: datetime, end_time: datetime,
                 metadata: dict = None):
        # Создание базового LogRecord
        super().__init__(
            name="solid_state_kinetics.aggregated",
            level=logging.INFO,
            pathname="aggregated",
            lineno=0,
            msg=summary,
            args=(),
            exc_info=None
        )
        
        # Дополнительные атрибуты для агрегированных записей
        self.original_records = original_records
        self.pattern_type = pattern_type
        self.start_time = start_time
        self.end_time = end_time
        self.aggregated_count = len(original_records)
        self.duration = (end_time - start_time).total_seconds()
        self.metadata = metadata or {}
        
        # Установка времени создания
        self.created = start_time.timestamp()

class AggregationEngine:
    """Движок агрегации логов в реальном времени с поддержкой табличного форматирования"""
    
    def __init__(self):
        self.aggregation_stats = {
            'total_processed': 0,
            'total_aggregated': 0,
            'patterns_processed': 0,
            'table_suitable_patterns': 0
        }
    
    def aggregate(self, records: List[LogRecord], 
                 patterns: List[PatternGroup]) -> List[logging.LogRecord]:
        """Агрегация записей на основе обнаруженных паттернов с улучшенным форматированием"""
        if not records:
            return []
        
        result = []
        processed_record_ids = set()
        
        # Обработка паттернов
        for pattern in patterns:
            aggregated_record = self._create_enhanced_aggregated_record(pattern)
            result.append(aggregated_record)
            
            # Отметка записей как обработанных
            for record in pattern.records:
                processed_record_ids.add(id(record))
            
            self.aggregation_stats['patterns_processed'] += 1
            
            # Подсчет паттернов, подходящих для табличного представления
            if pattern.metadata and pattern.metadata.get('table_suitable', False):
                self.aggregation_stats['table_suitable_patterns'] += 1
        
        # Добавление необработанных записей
        for record in records:
            if id(record) not in processed_record_ids:
                result.append(record.raw_record)
        
        # Сортировка по времени
        result.sort(key=lambda x: x.created)
        
        # Обновление статистики
        self.aggregation_stats['total_processed'] += len(records)
        self.aggregation_stats['total_aggregated'] += len(patterns)
        
        return result
    
    def _create_enhanced_aggregated_record(self, pattern: PatternGroup) -> AggregatedLogRecord:
        """Создание улучшенной агрегированной записи из паттерна"""
        summary = self._generate_enhanced_summary(pattern)
        
        return AggregatedLogRecord(
            summary=summary,
            original_records=pattern.records,
            pattern_type=pattern.pattern_type,
            start_time=pattern.start_time,
            end_time=pattern.end_time,
            metadata=pattern.metadata
        )
    
    def _generate_enhanced_summary(self, pattern: PatternGroup) -> str:
        """Генерация улучшенного краткого описания паттерна с использованием метаданных"""
        duration = pattern.duration.total_seconds()
        metadata = pattern.metadata or {}
        
        if pattern.pattern_type == "plot_lines_addition":
            line_names = metadata.get('line_names', [])
            unique_lines = metadata.get('unique_lines', len(line_names))
            avg_time = metadata.get('avg_time_per_line', 0)
            
            lines_preview = ', '.join(line_names[:3])
            if len(line_names) > 3:
                lines_preview += f"... (+{len(line_names)-3} more)"
            
            return (f"📊 AGGREGATED: Added {unique_lines} plot lines [{lines_preview}] "
                   f"({duration:.1f}s, avg: {avg_time:.1f}ms per line)")
        
        elif pattern.pattern_type == "cascade_component_initialization":
            components = metadata.get('components', [])
            cascade_depth = metadata.get('cascade_depth', len(components))
            avg_step_time = metadata.get('avg_step_time', 0)
            
            comp_chain = " → ".join(components[:5])
            if len(components) > 5:
                comp_chain += "..."
            
            return (f"🔧 AGGREGATED: Component initialization cascade "
                   f"[{comp_chain}] ({cascade_depth} steps, {duration:.1f}s, "
                   f"avg: {avg_step_time:.1f}ms per step)")
        
        elif pattern.pattern_type == "request_response_cycle":
            requests = metadata.get('requests_count', 0)
            responses = metadata.get('responses_count', 0)
            completion_ratio = metadata.get('completion_ratio', 0)
            
            return (f"🔄 AGGREGATED: Request-response cycle "
                   f"({requests} requests, {responses} responses, "
                   f"{completion_ratio:.1%} completion, {duration:.1f}s)")
        
        elif pattern.pattern_type == "file_operations":
            operation_types = metadata.get('operation_types', {})
            files_affected = metadata.get('files_affected', [])
            total_ops = metadata.get('total_operations', pattern.count)
            
            ops_summary = ', '.join(f"{op}: {count}" for op, count in operation_types.items())
            files_summary = f"{len(files_affected)} files" if files_affected else "unknown files"
            
            return (f"📁 AGGREGATED: File operations batch "
                   f"[{ops_summary}] affecting {files_summary} ({duration:.1f}s)")
        
        elif pattern.pattern_type == "gui_updates":
            levels_dist = metadata.get('levels_distribution', {})
            files_involved = metadata.get('files_involved', [])
            
            return (f"🖥️ AGGREGATED: GUI updates batch "
                   f"({pattern.count} updates across {len(files_involved)} files, {duration:.1f}s)")
        
        # Универсальное описание с метаданными
        priority = metadata.get('priority', 'unknown')
        table_suitable = metadata.get('table_suitable', False)
        table_indicator = "📋" if table_suitable else "📝"
        
        return (f"{table_indicator} AGGREGATED: {pattern.pattern_type} "
               f"({pattern.count} operations, {duration:.1f}s, priority: {priority})")
    
    def get_stats(self) -> dict:
        """Получение расширенной статистики агрегации"""
        stats = self.aggregation_stats.copy()
        
        # Добавление вычисляемых метрик
        if stats['patterns_processed'] > 0:
            stats['table_suitable_ratio'] = stats['table_suitable_patterns'] / stats['patterns_processed']
        else:
            stats['table_suitable_ratio'] = 0.0
        
        return stats

3. Обновленная конфигурация и интеграция
3.1 Расширенная конфигурация (config.py)
from dataclasses import dataclass
from typing import Dict, Any, List

@dataclass
class ErrorExpansionConfig:
    """Конфигурация для детализации ошибок"""
    enabled: bool = True
    context_lines: int = 5
    trace_depth: int = 10
    immediate_expansion: bool = True  # Немедленная обработка ошибок
    save_context: bool = True
    error_threshold_level: str = "WARNING"  # Минимальный уровень для детализации

@dataclass
class TabularFormattingConfig:
    """Конфигурация для табличного форматирования"""
    enabled: bool = True
    max_table_width: int = 120
    max_rows_per_table: int = 20
    ascii_tables: bool = True
    include_summaries: bool = True
    auto_format_patterns: List[str] = None  # Паттерны для автоматического табличного форматирования
    
    def __post_init__(self):
        if self.auto_format_patterns is None:
            self.auto_format_patterns = [
                "plot_lines_addition",
                "cascade_component_initialization", 
                "request_response_cycle",
                "file_operations"
            ]

@dataclass
class AggregationConfig:
    """Расширенная конфигурация агрегации в реальном времени"""
    
    # Параметры буферизации
    buffer_size: int = 1000
    flush_interval: float = 2.0
    context_buffer_size: int = 20
    
    # Параметры детекции паттернов
    min_pattern_entries: int = 3
    time_window_seconds: int = 5
    pattern_priority_threshold: str = "medium"
    
    # НОВЫЕ параметры для детализации ошибок
    error_expansion: ErrorExpansionConfig = None
    
    # НОВЫЕ параметры для табличного форматирования
    tabular_formatting: TabularFormattingConfig = None
    
    # Параметры производительности
    max_processing_time_ms: float = 100.0
    enable_async_processing: bool = False
    
    # Параметры вывода
    compression_threshold: float = 0.3  # Минимальный коэффициент сжатия
    output_format: str = "enhanced"  # "simple", "enhanced", "tabular"
    
    # Отладка и мониторинг
    enable_stats_logging: bool = True
    stats_interval_seconds: int = 60
    debug_mode: bool = False
    
    def __post_init__(self):
        if self.error_expansion is None:
            self.error_expansion = ErrorExpansionConfig()
        if self.tabular_formatting is None:
            self.tabular_formatting = TabularFormattingConfig()
    
    @classmethod
    def create_minimal(cls) -> 'AggregationConfig':
        """Создание минимальной конфигурации"""
        return cls(
            buffer_size=500,
            flush_interval=1.0,
            min_pattern_entries=2,
            error_expansion=ErrorExpansionConfig(
                enabled=True,
                context_lines=3,
                immediate_expansion=True
            ),
            tabular_formatting=TabularFormattingConfig(
                enabled=True,
                max_table_width=80,
                max_rows_per_table=10
            )
        )
    
    @classmethod
    def create_performance(cls) -> 'AggregationConfig':
        """Создание конфигурации для высокой производительности"""
        return cls(
            buffer_size=2000,
            flush_interval=0.5,
            min_pattern_entries=5,
            max_processing_time_ms=50.0,
            enable_async_processing=True,
            error_expansion=ErrorExpansionConfig(
                enabled=True,
                context_lines=2,
                immediate_expansion=False
            ),
            tabular_formatting=TabularFormattingConfig(
                enabled=False  # Отключено для производительности
            )
        )
    
    @classmethod
    def create_detailed(cls) -> 'AggregationConfig':
        """Создание конфигурации для максимальной детализации"""
        return cls(
            buffer_size=1500,
            flush_interval=3.0,
            min_pattern_entries=2,
            context_buffer_size=50,
            error_expansion=ErrorExpansionConfig(
                enabled=True,
                context_lines=10,
                trace_depth=20,
                immediate_expansion=True,
                save_context=True
            ),
            tabular_formatting=TabularFormattingConfig(
                enabled=True,
                max_table_width=150,
                max_rows_per_table=30,
                include_summaries=True
            ),
            enable_stats_logging=True,
            debug_mode=True
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Конвертация в словарь для сериализации"""
        return {
            'buffer_size': self.buffer_size,
            'flush_interval': self.flush_interval,
            'context_buffer_size': self.context_buffer_size,
            'min_pattern_entries': self.min_pattern_entries,
            'time_window_seconds': self.time_window_seconds,
            'pattern_priority_threshold': self.pattern_priority_threshold,
            'error_expansion': {
                'enabled': self.error_expansion.enabled,
                'context_lines': self.error_expansion.context_lines,
                'trace_depth': self.error_expansion.trace_depth,
                'immediate_expansion': self.error_expansion.immediate_expansion,
                'save_context': self.error_expansion.save_context,
                'error_threshold_level': self.error_expansion.error_threshold_level
            },
            'tabular_formatting': {
                'enabled': self.tabular_formatting.enabled,
                'max_table_width': self.tabular_formatting.max_table_width,
                'max_rows_per_table': self.tabular_formatting.max_rows_per_table,
                'ascii_tables': self.tabular_formatting.ascii_tables,
                'include_summaries': self.tabular_formatting.include_summaries,
                'auto_format_patterns': self.tabular_formatting.auto_format_patterns
            },
            'max_processing_time_ms': self.max_processing_time_ms,
            'enable_async_processing': self.enable_async_processing,
            'compression_threshold': self.compression_threshold,
            'output_format': self.output_format,
            'enable_stats_logging': self.enable_stats_logging,
            'stats_interval_seconds': self.stats_interval_seconds,
            'debug_mode': self.debug_mode
        }

3.2 Обновленная интеграция с LoggerManager
# Добавление в logger_config.py

from log_aggregator.realtime_handler import AggregatingHandler
from log_aggregator.config import AggregationConfig, ErrorExpansionConfig, TabularFormattingConfig

class LoggerManager:
    """Расширенный LoggerManager с поддержкой агрегации в реальном времени"""
    
    _aggregation_enabled = False
    _aggregation_config = None
    _aggregation_handlers = []
    
    @classmethod
    def configure_logging(
        cls,
        log_level: int = logging.DEBUG,
        console_level: Optional[int] = None,
        file_level: Optional[int] = None,
        log_file: Optional[str] = None,
        max_file_size: int = 10 * 1024 * 1024,
        backup_count: int = 5,
        enable_aggregation: bool = False,
        aggregation_config: Optional[dict] = None,
        enable_error_expansion: bool = True,  # НОВЫЙ параметр
        enable_tabular_format: bool = True,   # НОВЫЙ параметр
        aggregation_preset: str = "default", # НОВЫЙ параметр: "minimal", "performance", "detailed"
    ) -> None:
        """
        Конфигурация логирования с расширенной агрегацией в реальном времени.
        
        Новые параметры:
            enable_aggregation: Включить агрегацию логов в реальном времени
            aggregation_config: Настройки агрегации (buffer_size, flush_interval, etc.)
            enable_error_expansion: Включить детальный анализ ошибок
            enable_tabular_format: Включить табличное представление
            aggregation_preset: Предустановленная конфигурация ("minimal", "performance", "detailed")
        """
        # Существующий код остается без изменений...
        
        # Настройка агрегации (если включена)
        if enable_aggregation:
            cls._setup_enhanced_aggregation(
                root_logger, 
                aggregation_config or {}, 
                aggregation_preset,
                enable_error_expansion,
                enable_tabular_format
            )
        
        cls._configured = True
    
    @classmethod
    def _setup_enhanced_aggregation(cls, root_logger: logging.Logger, 
                                   config: dict, preset: str,
                                   enable_error_expansion: bool,
                                   enable_tabular_format: bool) -> None:
        """Настройка расширенной агрегации для существующих handlers"""
        
        # Создание конфигурации на основе пресета
        if preset == "minimal":
            cls._aggregation_config = AggregationConfig.create_minimal()
        elif preset == "performance":
            cls._aggregation_config = AggregationConfig.create_performance()
        elif preset == "detailed":
            cls._aggregation_config = AggregationConfig.create_detailed()
        else:
            cls._aggregation_config = AggregationConfig()
        
        # Применение пользовательских настроек
        for key, value in config.items():
            if hasattr(cls._aggregation_config, key):
                setattr(cls._aggregation_config, key, value)
        
        # Переопределение настроек компонентов
        cls._aggregation_config.error_expansion.enabled = enable_error_expansion
        cls._aggregation_config.tabular_formatting.enabled = enable_tabular_format
        
        # Замена существующих handlers на AggregatingHandler
        original_handlers = root_logger.handlers.copy()
        root_logger.handlers.clear()
        cls._aggregation_handlers = []
        
        for handler in original_handlers:
            aggregating_handler = AggregatingHandler(
                target_handler=handler,
                buffer_size=cls._aggregation_config.buffer_size,
                flush_interval=cls._aggregation_config.flush_interval,
                min_pattern_entries=cls._aggregation_config.min_pattern_entries,
                enabled=True,
                enable_error_expansion=cls._aggregation_config.error_expansion.enabled,
                enable_tabular_format=cls._aggregation_config.tabular_formatting.enabled,
                error_context_lines=cls._aggregation_config.error_expansion.context_lines
            )
            aggregating_handler.setLevel(handler.level)
            aggregating_handler.setFormatter(handler.formatter)
            root_logger.addHandler(aggregating_handler)
            cls._aggregation_handlers.append(aggregating_handler)
        
        cls._aggregation_enabled = True
        root_logger.info("Enhanced real-time log aggregation enabled with error expansion and tabular formatting")
        
        # Логирование конфигурации
        if cls._aggregation_config.debug_mode:
            root_logger.debug(f"Aggregation config: {cls._aggregation_config.to_dict()}")
    
    @classmethod
    def toggle_aggregation(cls, enabled: bool) -> None:
        """Включение/выключение агрегации в runtime"""
        if not cls._aggregation_enabled:
            return
        
        for handler in cls._aggregation_handlers:
            handler.toggle_aggregation(enabled)
        
        root_logger = logging.getLogger(cls._root_logger_name)
        status = "enabled" if enabled else "disabled"
        root_logger.info(f"Real-time aggregation {status}")
    
    @classmethod
    def toggle_error_expansion(cls, enabled: bool) -> None:
        """Включение/выключение детализации ошибок в runtime"""
        if not cls._aggregation_enabled:
            return
        
        for handler in cls._aggregation_handlers:
            handler.toggle_error_expansion(enabled)
        
        root_logger = logging.getLogger(cls._root_logger_name)
        status = "enabled" if enabled else "disabled"
        root_logger.info(f"Error expansion {status}")
    
    @classmethod
    def toggle_tabular_format(cls, enabled: bool) -> None:
        """Включение/выключение табличного формата в runtime"""
        if not cls._aggregation_enabled:
            return
        
        for handler in cls._aggregation_handlers:
            handler.toggle_tabular_format(enabled)
        
        root_logger = logging.getLogger(cls._root_logger_name)
        status = "enabled" if enabled else "disabled"
        root_logger.info(f"Tabular formatting {status}")
    
    @classmethod
    def get_aggregation_stats(cls) -> dict:
        """Получение расширенной статистики агрегации"""
        if not cls._aggregation_enabled:
            return {}
        
        combined_stats = {
            'handlers': {},
            'total_stats': {
                'total_records': 0,
                'aggregated_records': 0,
                'patterns_detected': 0,
                'errors_expanded': 0,
                'tables_generated': 0,
                'buffer_flushes': 0
            }
        }
        
        for i, handler in enumerate(cls._aggregation_handlers):
            handler_stats = handler.get_stats()
            combined_stats['handlers'][f'handler_{i}'] = handler_stats
            
            # Суммирование общей статистики
            for key in combined_stats['total_stats']:
                if key in handler_stats:
                    combined_stats['total_stats'][key] += handler_stats[key]
        
        # Добавление вычисляемых метрик
        total_processed = combined_stats['total_stats']['total_records']
        if total_processed > 0:
            combined_stats['total_stats']['compression_ratio'] = (
                1 - combined_stats['total_stats']['aggregated_records'] / total_processed
            )
            combined_stats['total_stats']['error_expansion_ratio'] = (
                combined_stats['total_stats']['errors_expanded'] / total_processed
            )
        
        return combined_stats
    
    @classmethod
    def export_aggregation_config(cls) -> dict:
        """Экспорт текущей конфигурации агрегации"""
        if cls._aggregation_config:
            return cls._aggregation_config.to_dict()
        return {}
    
    @classmethod
    def update_aggregation_config(cls, config_updates: dict) -> None:
        """Обновление конфигурации агрегации в runtime"""
        if not cls._aggregation_enabled or not cls._aggregation_config:
            return
        
        # Обновление конфигурации
        for key, value in config_updates.items():
            if hasattr(cls._aggregation_config, key):
                setattr(cls._aggregation_config, key, value)
        
        # Применение изменений к handlers
        for handler in cls._aggregation_handlers:
            if 'buffer_size' in config_updates:
                handler.buffer_manager.max_size = config_updates['buffer_size']
            if 'flush_interval' in config_updates:
                handler.buffer_manager.flush_interval = config_updates['flush_interval']
            # Добавить другие обновляемые параметры по необходимости
        
        root_logger = logging.getLogger(cls._root_logger_name)
        root_logger.info("Aggregation configuration updated")

4. Примеры использования новых компонентов
4.1 Пример детального вывода ошибки

Входные логи:

2024-01-15 10:30:45 INFO Loading configuration file...
2024-01-15 10:30:45 INFO Initializing GUI components
2024-01-15 10:30:46 INFO Setting up plot window
2024-01-15 10:30:46 ERROR Failed to load data file: /path/to/data.csv


Вывод с ErrorExpansionEngine:

================================================================================
🚨 DETAILED ERROR ANALYSIS - ERROR
================================================================================
📍 Location: data_loader.py:45
⏰ Time: 2024-01-15 10:30:46
💬 Message: Failed to load data file: /path/to/data.csv

📋 PRECEDING CONTEXT:
----------------------------------------
  1. [INFO] Setting up plot window (0.2s ago)
  2. [INFO] Initializing GUI components (1.0s ago)
  3. [INFO] Loading configuration file... (1.1s ago)

🔗 RELATED OPERATIONS:
----------------------------------------
  1. [INFO] data_loader.py:23 - Opening file dialog
  2. [INFO] data_loader.py:35 - User selected file: /path/to/data.csv

💡 SUGGESTED ACTIONS:
----------------------------------------
  1. Проверьте существование файла
  2. Убедитесь в правильности пути
  3. Проверьте права доступа
  4. Проверьте код в файле data_loader.py:45

================================================================================

4.2 Пример табличного представления для добавления линий

Входные логи:

2024-01-15 10:31:00 INFO Adding a new line 'Temperature_1' to the plot
2024-01-15 10:31:00 INFO Adding a new line 'Temperature_2' to the plot
2024-01-15 10:31:00 INFO Adding a new line 'Pressure_1' to the plot
2024-01-15 10:31:01 INFO Adding a new line 'Pressure_2' to the plot
2024-01-15 10:31:01 INFO Adding a new line 'Humidity' to the plot


Вывод с TabularFormatter:

┌──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ 📊 Plot Lines Addition Summary                                                                                               │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ #  │ Line Name      │ Time        │ Duration (ms) │ Status     │
├────┼────────────────┼─────────────┼───────────────┼────────────┤
│ 1  │ Temperature_1  │ +0.0ms      │ 0.0           │ ✅ Success │
│ 2  │ Temperature_2  │ +150.2ms    │ 150.2         │ ✅ Success │
│ 3  │ Pressure_1     │ +320.5ms    │ 320.5         │ ✅ Success │
│ 4  │ Pressure_2     │ +890.1ms    │ 890.1         │ ✅ Success │
│ 5  │ Humidity       │ +1200.3ms   │ 1200.3        │ ✅ Success │
└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
📊 Total: 5 lines added in 1200.3ms (avg: 240.1ms per line)

4.3 Пример табличного представления для каскадной инициализации

Входные логи:

2024-01-15 10:32:00 INFO Initializing ConfigManager
2024-01-15 10:32:00 INFO Initializing DatabaseConnection
2024-01-15 10:32:01 INFO Initializing PlotEngine
2024-01-15 10:32:01 INFO Initializing GUIManager
2024-01-15 10:32:02 INFO Initializing EventHandler


Вывод с TabularFormatter:

┌──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ 🔧 Component Initialization Cascade                                                                                         │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ Step │ Component           │ Time        │ Duration (ms) │ Status │
├──────┼─────────────────────┼─────────────┼───────────────┼────────┤
│ 1    │ ConfigManager       │ +0.0ms      │ 0.0           │ ✅ OK  │
│ 2    │ DatabaseConnection  │ +120.5ms    │ 120.5         │ ✅ OK  │
│ 3    │ PlotEngine          │ +650.2ms    │ 529.7         │ ✅ OK  │
│ 4    │ GUIManager          │ +1100.8ms   │ 450.6         │ ✅ OK  │
│ 5    │ EventHandler        │ +1800.3ms   │ 699.5         │ ✅ OK  │
└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
📊 Initialization cascade: 5 components in 1800.3ms

4.4 Пример табличного представления для файловых операций

Входные логи:

2024-01-15 10:33:00 INFO Loading experiment_1.json
2024-01-15 10:33:01 INFO Saving results to output.csv
2024-01-15 10:33:01 INFO Loading calibration.dat
2024-01-15 10:33:02 INFO Saving backup to backup_001.json


Вывод с TabularFormatter:

┌──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ 📁 File Operations Summary                                                                                                   │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ #  │ Operation │ File                │ Time         │ Status     │
├────┼───────────┼─────────────────────┼──────────────┼────────────┤
│ 1  │ Load      │ experiment_1.json   │ 10:33:00.000 │ ✅ Success │
│ 2  │ Save      │ output.csv          │ 10:33:01.200 │ ✅ Success │
│ 3  │ Load      │ calibration.dat     │ 10:33:01.850 │ ✅ Success │
│ 4  │ Save      │ backup_001.json     │ 10:33:02.100 │ ✅ Success │
└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
📊 File operations: load: 2, save: 2

5. Workflow обработки с новыми компонентами
5.1 Общий workflow
Входящий LogRecord
        │
        ▼
┌─────────────────┐
│ AggregatingHandler │
└─────────────────┘
        │
        ▼
    Проверка уровня
        │
    ┌───▼───┐
    │ ERROR/│ ──YES──► ErrorExpansionEngine ──► Немедленный вывод
    │WARNING│                                   с расширенным контекстом
    └───┬───┘
        │ NO
        ▼
┌─────────────────┐
│  BufferManager  │ ◄──── Сохранение контекста
└─────────────────┘       для будущих ошибок
        │
        ▼
    Проверка условий
    агрегации
        │
        ▼
┌─────────────────┐
│ PatternDetector │ ──► Обнаружение паттернов
└─────────────────┘     с метаданными
        │
        ▼
┌─────────────────┐
│AggregationEngine│ ──► Создание агрегированных
└─────────────────┘     записей
        │
        ▼
┌─────────────────┐
│TabularFormatter │ ──► Создание таблиц для
└─────────────────┘     подходящих паттернов
        │
        ▼
    Вывод в target_handler

5.2 Workflow обработки ошибок
ERROR/WARNING LogRecord
        │
        ▼
┌─────────────────────┐
│ ErrorExpansionEngine │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Получение контекста │ ◄── BufferManager.get_recent_context()
│ из BufferManager    │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Анализ предшествую- │
│ щих операций        │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Поиск связанных     │
│ операций            │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Классификация       │
│ типа ошибки         │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Генерация           │
│ рекомендаций        │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Создание расширен-  │
│ ного LogRecord      │
└─────────────────────┘
        │
        ▼
    Немедленный вывод

5.3 Workflow табличного форматирования
Обнаруженные паттерны
        │
        ▼
┌─────────────────────┐
│ TabularFormatter    │
└─────────────────────┘
        │
        ▼
    Проверка table_suitable
    в метаданных паттерна
        │
    ┌───▼───┐
    │ TRUE  │ ──► Создание TableData
    └───┬───┘
        │ FALSE
        ▼
    Пропуск паттерна
        │
        ▼
┌─────────────────────┐
│ Определение типа    │
│ таблицы по паттерну │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Извлечение данных   │
│ для таблицы         │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Расчет ширины       │
│ колонок             │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Форматирование      │
│ ASCII таблицы       │
└─────────────────────┘
        │
        ▼
┌─────────────────────┐
│ Создание TableRecord│
│ LogRecord           │
└─────────────────────┘
        │
        ▼
    Добавление к выводу

6. Тестирование и валидация
6.1 Тесты для ErrorExpansionEngine
# tests/test_error_expansion.py

import unittest
from datetime import datetime, timedelta
from log_aggregator.error_expansion import ErrorExpansionEngine, ErrorContext
from log_aggregator.realtime_handler import LogRecord

class TestErrorExpansionEngine(unittest.TestCase):
    
    def setUp(self):
        self.engine = ErrorExpansionEngine(context_lines=3, trace_depth=5)
        self.base_time = datetime.now()
    
    def test_error_expansion_with_context(self):
        """Тест расширения ошибки с контекстом"""
        # Создание контекстных записей
        context_records = [
            LogRecord(
                timestamp=self.base_time - timedelta(seconds=2),
                level="INFO",
                filename="test.py",
                line_number=10,
                message="Loading configuration",
                raw_record=None
            ),
            LogRecord(
                timestamp=self.base_time - timedelta(seconds=1),
                level="INFO", 
                filename="test.py",
                line_number=15,
                message="Initializing components",
                raw_record=None
            )
        ]
        
        # Создание записи ошибки
        error_record = LogRecord(
            timestamp=self.base_time,
            level="ERROR",
            filename="test.py", 
            line_number=20,
            message="Failed to load file not found",
            raw_record=None
        )
        
        # Расширение ошибки
        expanded_record = self.engine.expand_error(error_record, context_records)
        
        # Проверки
        self.assertIn("DETAILED ERROR ANALYSIS", expanded_record.getMessage())
        self.assertIn("PRECEDING CONTEXT", expanded_record.getMessage())
        self.assertIn("Loading configuration", expanded_record.getMessage())
        self.assertIn("SUGGESTED ACTIONS", expanded_record.getMessage())
        self.assertEqual(self.engine.stats['errors_processed'], 1)
    
    def test_error_classification(self):
        """Тест классификации ошибок"""
        test_cases = [
            ("file not found error", "file_not_found"),
            ("memory allocation failed", "memory_error"),
            ("widget rendering error", "gui_error"),
            ("division by zero", "calculation_error"),
            ("unknown error type", "unknown")
        ]
        
        for message, expected_type in test_cases:
            error_record = LogRecord(
                timestamp=self.base_time,
                level="ERROR",
                filename="test.py",
                line_number=1,
                message=message,
                raw_record=None
            )
            
            error_type = self.engine._classify_error(error_record)
            self.assertEqual(error_type, expected_type)
    
    def test_suggestions_generation(self):
        """Тест генерации рекомендаций"""
        error_record = LogRecord(
            timestamp=self.base_time,
            level="ERROR",
            filename="data_loader.py",
            line_number=45,
            message="file not found",
            raw_record=None
        )
        
        suggestions = self.engine._generate_suggestions("file_not_found", error_record)
        
        self.assertIn("Проверьте существование файла", suggestions)
        self.assertIn("data_loader.py:45", suggestions[-1])

6.2 Тесты для TabularFormatter
# tests/test_tabular_formatter.py

import unittest
from datetime import datetime, timedelta
from log_aggregator.tabular_formatter import TabularFormatter, TableData
from log_aggregator.pattern_detector import PatternGroup
from log_aggregator.realtime_handler import LogRecord

class TestTabularFormatter(unittest.TestCase):
    
    def setUp(self):
        self.formatter = TabularFormatter(max_table_width=80, max_rows_per_table=10)
        self.base_time = datetime.now()
    
    def test_plot_lines_table_creation(self):
        """Тест создания таблицы для добавления линий"""
        # Создание записей для паттерна
        records = []
        for i, line_name in enumerate(['Temperature_1', 'Temperature_2', 'Pressure_1']):
            record = LogRecord(
                timestamp=self.base_time + timedelta(milliseconds=i*100),
                level="INFO",
                filename="plot.py",
                line_number=25,
                message=f"Adding a new line '{line_name}' to the plot",
                raw_record=None
            )
            records.append(record)
        
        # Создание паттерна
        pattern = PatternGroup(
            pattern_type="plot_lines_addition",
            records=records,
            start_time=records[0].timestamp,
            end_time=records[-1].timestamp,
            metadata={'table_suitable': True}
        )
        
        # Создание таблицы
        table_data = self.formatter._create_plot_lines_table(pattern)
        
        # Проверки
        self.assertEqual(table_data.title, "📊 Plot Lines Addition Summary")
        self.assertEqual(len(table_data.rows), 3)
        self.assertIn("Temperature_1", table_data.rows[0][1])
        self.assertIn("Temperature_2", table_data.rows[1][1])
        self.assertIn("Pressure_1", table_data.rows[2][1])
        self.assertIn("Total: 3 lines added", table_data.summary)
    
    def test_ascii_table_formatting(self):
        """Тест ASCII форматирования таблицы"""
        table_data = TableData(
            title="Test Table",
            headers=["Col1", "Col2", "Col3"],
            rows=[
                ["Row1", "Data1", "Value1"],
                ["Row2", "Data2", "Value2"]
            ],
            summary="Test summary"
        )
        
        ascii_table = self.formatter._format_ascii_table(table_data)
        
        # Проверки
        self.assertIn("Test Table", ascii_table)
        self.assertIn("Col1", ascii_table)
        self.assertIn("Row1", ascii_table)
        self.assertIn("Test summary", ascii_table)
        self.assertIn("┌", ascii_table)  # Проверка границ таблицы
        self.assertIn("└", ascii_table)
    
    def test_column_width_calculation(self):
        """Тест расчета ширины колонок"""
        table_data = TableData(
            title="Test",
            headers=["Short", "Very Long Header Name", "Med"],
            rows=[
                ["A", "Short", "Medium Length"],
                ["Very Long Data", "B", "C"]
            ]
        )
        
        col_widths = self.formatter._calculate_column_widths(table_data)
        
        # Проверки
        self.assertEqual(len(col_widths), 3)
        self.assertGreaterEqual(col_widths[0], len("Very Long Data"))
        self.assertGreaterEqual(col_widths[1], len("Very Long Header Name"))
        self.assertGreaterEqual(col_widths[2], len("Medium Length"))

6.3 Интеграционные тесты
# tests/test_integration.py

import unittest
import logging
from datetime import datetime, timedelta
from log_aggregator.realtime_handler import AggregatingHandler
from log_aggregator.config import AggregationConfig

class TestIntegration(unittest.TestCase):
    
    def setUp(self):
        # Создание тестового handler
        self.target_handler = logging.StreamHandler()
        
        # Создание конфигурации
        config = AggregationConfig.create_detailed()
        
        # Создание AggregatingHandler
        self.aggregating_handler = AggregatingHandler(
            target_handler=self.target_handler,
            buffer_size=config.buffer_size,
            flush_interval=config.flush_interval,
            min_pattern_entries=config.min_pattern_entries,
            enabled=True,
            enable_error_expansion=config.error_expansion.enabled,
            enable_tabular_format=config.tabular_formatting.enabled,
            error_context_lines=config.error_expansion.context_lines
        )
    
    def test_error_immediate_processing(self):
        """Тест немедленной обработки ошибок"""
        # Создание контекстных записей
        info_record = logging.LogRecord(
            name="test",
            level=logging.INFO,
            pathname="test.py",
            lineno=10,
            msg="Loading data",
            args=(),
            exc_info=None
        )
        
        # Добавление контекста
        self.aggregating_handler.emit(info_record)
        
        # Создание записи ошибки
        error_record = logging.LogRecord(
            name="test",
            level=logging.ERROR,
            pathname="test.py", 
            lineno=15,
            msg="File not found",
            args=(),
            exc_info=None
        )
        
        # Обработка ошибки (должна быть немедленной)
        initial_errors = self.aggregating_handler.stats['errors_expanded']
        self.aggregating_handler.emit(error_record)
        
        # Проверка, что ошибка была обработана немедленно
        self.assertEqual(
            self.aggregating_handler.stats['errors_expanded'], 
            initial_errors + 1
        )
    
    def test_pattern_detection_and_table_generation(self):
        """Тест детекции паттернов и генерации таблиц"""
        # Создание серии записей для паттерна
        base_time = datetime.now().timestamp()
        
        for i in range(5):
            record = logging.LogRecord(
                name="test",
                level=logging.INFO,
                pathname="plot.py",
                lineno=20,
                msg=f"Adding a new line 'Line_{i}' to the plot",
                args=(),
                exc_info=None
            )
            record.created = base_time + i * 0.1
            self.aggregating_handler.emit(record)
        
        # Принудительная обработка буфера
        self.aggregating_handler._process_buffer()
        
        # Проверка статистики
        stats = self.aggregating_handler.get_stats()
        self.assertGreater(stats['patterns_detected'], 0)
        self.assertGreater(stats['tables_generated'], 0)
    
    def test_configuration_updates(self):
        """Тест обновления конфигурации в runtime"""
        # Проверка начального состояния
        self.assertTrue(self.aggregating_handler.enable_error_expansion)
        self.assertTrue(self.aggregating_handler.enable_tabular_format)
        
        # Отключение компонентов
        self.aggregating_handler.toggle_error_expansion(False)
        self.aggregating_handler.toggle_tabular_format(False)
        
        # Проверка изменений
        self.assertFalse(self.aggregating_handler.enable_error_expansion)
        self.assertFalse(self.aggregating_handler.enable_tabular_format)
        
        # Включение обратно
        self.aggregating_handler.toggle_error_expansion(True)
        self.aggregating_handler.toggle_tabular_format(True)
        
        # Проверка восстановления
        self.assertTrue(self.aggregating_handler.enable_error_expansion)
        self.assertTrue(self.aggregating_handler.enable_tabular_format)

7. Заключение

Обновленная архитектура модуля агрегации логов теперь включает два важных новых компонента:

7.1 ErrorExpansionEngine
Автоматическое расширение контекста для записей уровня WARNING/ERROR
Немедленная обработка критических ошибок без буферизации
Интеллектуальный анализ предшествующих операций и связанных событий
Генерация рекомендаций на основе типа ошибки
Трассировка ошибок с сохранением контекста
7.2 TabularFormatter
Автоматическое преобразование агрегированных паттернов в таблицы
Поддержка различных типов таблиц для разных паттернов
ASCII форматирование для консольного вывода
Адаптивная ширина колонок с учетом ограничений терминала
Сводная статистика для каждой таблицы
7.3 Ключевые улучшения
Детализация ошибок: Критические события получают расширенный контекст автоматически
Структурированный вывод: Агрегированные данные представляются в удобном табличном формате
Гибкая конфигурация: Возможность включения/выключения компонентов независимо
Производительность: Ошибки обрабатываются немедленно, не влияя на основной поток агрегации
Расширяемость: Легкое добавление новых типов таблиц и обработчиков ошибок

Модуль остается полностью обратно совместимым и может работать как с новыми компонентами, так и без них, в зависимости от конфигурации.